{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSjfQEccOOdb"
   },
   "source": [
    "# Analysis of an international research paper mill\n",
    "\n",
    "The purpose of the code below is to prove several papers have been given false writers’ names in exchange for money. The code below uses a website to show all the papers that are going to be published/have been published that we know of, as of July 14, 2021. We believe this is one of many websites that allow for people to get their names on papers, so although this will capture some of the papers, it will not get all of the papers that have false names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp_wkUdPMa7h",
    "tags": []
   },
   "source": [
    "## Workspace set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXeN41KtN_HC",
    "outputId": "5718c70b-b0dd-4bed-e2fb-fc4854a75d17"
   },
   "outputs": [],
   "source": [
    "#from serpapi import GoogleSearch\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import codecs\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request \n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGEVroGZf0ow"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGKNBRkS7hcx",
    "tags": []
   },
   "source": [
    "# Data retrieval\n",
    "\n",
    "This section is used to retrieve the contracts from the research paper mill using the first contract as the starting link:  http://123mi.ru/1/contract.php?r=1&n=1&m=1.  This code then increments the contracts for each author position for each unique paper.  The code parses the title and cost of the author position (in rubles), which is saved to a data frame, along with the link to the respective contract.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is also optional if you have already saved the outputted dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gS22TtydhCzO",
    "outputId": "899bb632-f44a-488a-c380-4d1ec91bff67",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/7gdsmfk52dl1hmwgr61mg_gm0000gq/T/ipykernel_32151/1172798161.py:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_paper = df_paper.append(df)\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: NOT FOUND",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m author_position \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# The starting link is http://123mi.ru/1/contract.php?r=1&n=1&m=1\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# A backup for the web archive is: https://web.archive.org/web/20201031161224/http://123mi.ru/1/contract.php?r=1&n=1000&m=1\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# This changes the link every loop to ajust to what we need. \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://web.archive.org/web/20201031161224/http://123mi.ru/1/contract.php?r=1&n=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m&m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(contract_number, author_position)\n\u001b[0;32m---> 12\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mURL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Parse the html file\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     htmlParse \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anonimizer/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anonimizer/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    530\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anonimizer/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anonimizer/lib/python3.8/urllib/request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    568\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anonimizer/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anonimizer/lib/python3.8/urllib/request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: NOT FOUND"
     ]
    }
   ],
   "source": [
    "#prep\n",
    "df_paper = pd.DataFrame()\n",
    "\n",
    "# Set the number of papers / contracts to pull \n",
    "for contract_number in range(1000, 1009):\n",
    "    # Set the number of author positions to pull\n",
    "    for author_position in range(1, 8):\n",
    "        # The starting link is http://123mi.ru/1/contract.php?r=1&n=1&m=1\n",
    "        # A backup for the web archive is: https://web.archive.org/web/20201031161224/http://123mi.ru/1/contract.php?r=1&n=1000&m=1\n",
    "        # This changes the link every loop to ajust to what we need. \n",
    "        URL = 'https://web.archive.org/web/20201031161224/http://123mi.ru/1/contract.php?r=1&n={}&m={}'.format(contract_number, author_position)\n",
    "        html = urllib.request.urlopen(URL)\n",
    "        \n",
    "        # Parse the html file\n",
    "        htmlParse = BeautifulSoup(html, 'html.parser')\n",
    "        for par in htmlParse.find_all(\"h1\"):\n",
    "            for para in par.find_all(\"font\"):\n",
    "                coun = para.get_text()\n",
    "                contract = coun.split('.')\n",
    "                contract = contract[0]\n",
    "                #print(coun)\n",
    "        # Get all the paragraphs\n",
    "        count = 0\n",
    "        for para in htmlParse.find_all(\"p\"):\n",
    "            count += 1\n",
    "            # This is a semi hard coded way to find all the paragraphs and check how many it has looped over.\n",
    "            #print(para)\n",
    "            if count == 2:\n",
    "                # Find/retreave the Russian titles\n",
    "                scopus_Russian_title = para.get_text()\n",
    "                #everything = scopus_Russian_title\n",
    "                Russian_title = scopus_Russian_title.split('«')\n",
    "                Russian_title_all = Russian_title[1].split('»')\n",
    "                #print(Russian_title[0])\n",
    "                # This makes sure there isn't a traceback that happens here because the code can take a while. \n",
    "                # The point of this code it to split the data to make it easier to find the nessary parts. \n",
    "                try:\n",
    "                    scopus = Russian_title[0].split('(')\n",
    "                    scopus = scopus[2].split(')')\n",
    "                    scopus = scopus[0]\n",
    "                    \n",
    "                except:\n",
    "                    scopus = scopus_Russian_title\n",
    "                    Russian_title = Russian_title[1].split('»')\n",
    "                    \n",
    "                # This is used to check if there is multiple titles, because they are always seperated by a \\n. \n",
    "                if '\\n' in Russian_title_all[0]:\n",
    "                    split = Russian_title_all[0].split('\\n')\n",
    "                    rushed_title = split[0]\n",
    "                    english_titl = split[1]\n",
    "                    \n",
    "                else:\n",
    "                    # if there is not a \\n, then the code will just take the title thats there. \n",
    "                    rushed_title = Russian_title_all[0]\n",
    "                    #print(Russian_title[0])\n",
    "                    english_titl = 'no suspected english title'\n",
    "                    \n",
    "            if count == 2:\n",
    "                web = para.get_text()\n",
    "                try:\n",
    "                    if 'Web of Science' in web:\n",
    "                        web = web.split('(')\n",
    "                        web = web[3]\n",
    "                        web = web.split(')')\n",
    "                        web = web[0]\n",
    "                        print(web)\n",
    "                        contains = web\n",
    "                except:\n",
    "                    print(web)\n",
    "                else:\n",
    "                    contains = False\n",
    "                \n",
    "            if count == 7:\n",
    "                # Retrieve the price from the data\n",
    "                Price_Ruble = para.get_text()\n",
    "                #print(Price_Ruble)\n",
    "                Price_Ruble = Price_Ruble.split('<font color=\"#0000FF\">')\n",
    "                Price_Ruble = Price_Ruble[0].split('Общая стоимость услуг, выполняемых Исполнителем в рамках настоящего Договора, составляет ')\n",
    "                Price_Ruble = Price_Ruble[1].split(' (')\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        # This takes all the data and puts it into a large dataframe.\n",
    "        if count == 7:\n",
    "            break\n",
    "        data = {'contract_link': [URL], 'Contract_number' : contract, \"Base_title\" : [Russian_title_all[0]], 'First_title': [rushed_title], 'Suspected_second_title' : [english_titl], 'Price_Ruble': Price_Ruble[0], 'contract_number': coun, 'Scopus': scopus, 'Web_of_science' : contains}\n",
    "        #print(data)\n",
    "        df = pd.DataFrame(data) \n",
    "        df_paper = df_paper.append(df)\n",
    "df_paper = df_paper.reset_index()\n",
    "df_paper = df_paper.drop('index', axis = 1)\n",
    "# This can also be removed to not show the dataframe at the end. \n",
    "df_paper\n",
    "df_paper.to_excel('raw_df.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973.1\n",
      "1973.2\n",
      "1973.3\n",
      "1973.4\n",
      "1973.5\n",
      "1974.1\n",
      "1975.1\n",
      "1976.1\n",
      "1977.1\n",
      "1978.1\n",
      "1979.1\n",
      "1980.1\n",
      "1981.1\n",
      "1982.1\n",
      "1983.1\n",
      "1984.1\n",
      "1985.1\n",
      "1986.1\n",
      "1987.1\n",
      "1988.1\n",
      "1989.1\n",
      "1990.1\n",
      "1991.1\n",
      "1992.1\n",
      "1993.1\n",
      "1994.1\n",
      "1995.1\n",
      "1996.1\n",
      "1997.1\n",
      "1998.1\n",
      "1999.1\n",
      "2000.1\n",
      "2001.1\n",
      "2002.1\n",
      "2003.1\n",
      "2004.1\n",
      "2005.1\n",
      "2006.1\n",
      "2007.1\n",
      "2008.1\n",
      "2009.1\n",
      "2010.1\n",
      "2011.1\n",
      "2012.1\n",
      "2013.1\n",
      "2014.1\n",
      "2015.1\n",
      "2016.1\n",
      "2017.1\n",
      "2018.1\n",
      "2019.1\n",
      "2020.1\n",
      "2021.1\n",
      "2022.1\n",
      "2023.1\n",
      "2024.1"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a3be1bed73a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"font\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mcoun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpara\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Get all the paragraphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                 \u001b[1;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    489\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    490\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is the same code as above, except it is used to get the updated data from the contracts \n",
    "# This code can be run as many times as needed\n",
    "end = df_paper.iloc[-1]['Contract_number']\n",
    "end = int(end)\n",
    "for contract_number in range(end+1, end+100):\n",
    "    # Set the number of author positions to pull\n",
    "    for author_position in range(1, 8):\n",
    "        # The starting link is http://123mi.ru/1/contract.php?r=1&n=1&m=1\n",
    "        # A backup for the web archive is: https://web.archive.org/web/20201031161224/http://123mi.ru/1/contract.php?r=1&n=1000&m=1\n",
    "        # This changes the link every loop to ajust to what we need. \n",
    "        URL = 'http://123mi.ru/1/contract.php?r=1&n={}&m={}'.format(contract_number, author_position)\n",
    "        html = urllib.request.urlopen(URL)\n",
    "        \n",
    "        # Parse the html file\n",
    "        htmlParse = BeautifulSoup(html, 'html.parser')\n",
    "        for par in htmlParse.find_all(\"h1\"):\n",
    "            for para in par.find_all(\"font\"):\n",
    "                coun = para.get_text()\n",
    "                contract = counx.split('.')\n",
    "                contract = contract[0]\n",
    "        # Get all the paragraphs\n",
    "        count = 0\n",
    "        for para in htmlParse.find_all(\"p\"):\n",
    "            count += 1\n",
    "            # This is a semi hard coded way to find all the paragraphs and check how many it has looped over.\n",
    "            #print(para)\n",
    "            if count == 2:\n",
    "                # Find/retreave the Russian titles\n",
    "                scopus_Russian_title = para.get_text()\n",
    "                #everything = scopus_Russian_title\n",
    "                Russian_title = scopus_Russian_title.split('«')\n",
    "                Russian_title_all = Russian_title[1].split('»')\n",
    "                #print(Russian_title[0])\n",
    "                # This makes sure there isn't a traceback that happens here because the code can take a while. \n",
    "                # The point of this code it to split the data to make it easier to find the nessary parts. \n",
    "                try:\n",
    "                    scopus = Russian_title[0].split('(')\n",
    "                    scopus = scopus[2].split(')')\n",
    "                    scopus = scopus[0]\n",
    "                    \n",
    "                except:\n",
    "                    scopus = scopus_Russian_title\n",
    "                    Russian_title = Russian_title[1].split('»')\n",
    "                    \n",
    "                # This is used to check if there is multiple titles, because they are always seperated by a \\n. \n",
    "                if '\\n' in Russian_title_all[0]:\n",
    "                    split = Russian_title_all[0].split('\\n')\n",
    "                    rushed_title = split[0]\n",
    "                    english_titl = split[1]\n",
    "                    \n",
    "                else:\n",
    "                    # if there is not a \\n, then the code will just take the title thats there. \n",
    "                    rushed_title = Russian_title_all[0]\n",
    "                    #print(Russian_title[0])\n",
    "                    english_titl = 'no suspected english title'\n",
    "                    \n",
    "            if count == 2:\n",
    "                web = para.get_text()\n",
    "                if 'Web of Science' in web:\n",
    "                    contains = True\n",
    "                else:\n",
    "                    contains = False\n",
    "                \n",
    "            if count == 7:\n",
    "                # Retrieve the price from the data\n",
    "                Price_Ruble = para.get_text()\n",
    "                #print(Price_Ruble)\n",
    "                Price_Ruble = Price_Ruble.split('<font color=\"#0000FF\">')\n",
    "                Price_Ruble = Price_Ruble[0].split('Общая стоимость услуг, выполняемых Исполнителем в рамках настоящего Договора, составляет ')\n",
    "                Price_Ruble = Price_Ruble[1].split(' (')\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        # This takes all the data and puts it into a large dataframe.\n",
    "        if count == 7:\n",
    "            break\n",
    "        data = {'contract_link': [URL], \"Base_title\" : [Russian_title_all[0]], 'First_title': [rushed_title], 'Suspected_second_title' : [english_titl], 'Price_Ruble': Price_Ruble[0], 'contract_number': coun, 'Scopus': scopus, 'Web_of_science' : contains}\n",
    "        #print(data)\n",
    "        df = pd.DataFrame(data) \n",
    "        df_paper = df_paper.append(df)\n",
    "# This can also be removed to not show the dataframe at the end. \n",
    "df_paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "CI9rI4Lej4wu",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-4896e5380c85>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_paper['length'] = df_paper.First_title.str.len()\n",
      "<ipython-input-18-4896e5380c85>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_paper['length'] = df_paper['length'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Backup save of data before cleaning\n",
    "df_paper.to_excel('raw_df.xlsx')\n",
    "# This replaces the punctuation in the columns\n",
    "df_paper[\"Base_title\"] = df_paper['Base_title'].str.replace('[^\\w\\s]','', regex=True)\n",
    "df_paper[\"First_title\"] = df_paper['First_title'].str.replace('[^\\w\\s]','', regex=True)\n",
    "df_paper[\"Suspected_second_title\"] = df_paper['Suspected_second_title'].str.replace('[^\\w\\s]','', regex=True)\n",
    "df_paper = df_paper[df_paper['Base_title'].astype(bool)]\n",
    "# Identify and remove contracts not successfully retrieved\n",
    "df_paper['length'] = df_paper.First_title.str.len()\n",
    "df_paper['length'] = df_paper['length'].astype(int)  \n",
    "df_unconfirmed = df_paper[df_paper.length < 20]\n",
    "df_paper = df_paper[df_paper.length > 20]\n",
    "# This checks if the price has been colected, and adds removes it if it has not\n",
    "# I have this in place because empty contracts will have a price of 0\n",
    "df_paper = df_paper[df_paper['Price_Ruble'] !='0']\n",
    "# Construct data frames\n",
    "test = df_paper\n",
    "df_titles = test.drop_duplicates(subset=['Base_title'])\n",
    "df_titles = df_titles[['Contract_number', 'Base_title']]\n",
    "df_authors = df_paper\n",
    "# Calculated on 8/5/2021\n",
    "df_authors['Price_Ruble'] = df_authors['Price_Ruble'].astype('float')\n",
    "df_authors['USD'] = df_authors['Price_Ruble'] * .014\n",
    "# Backup index reset\n",
    "df_paper = df_paper.reset_index()\n",
    "df_paper_original = df_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Website Direct Scrape\n",
    "\n",
    "This is used to scrape the website and find the all the papers that are still being sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paper = pd.read_excel('raw_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>contract_link</th>\n",
       "      <th>Contract_number</th>\n",
       "      <th>Base_title</th>\n",
       "      <th>First_title</th>\n",
       "      <th>Suspected_second_title</th>\n",
       "      <th>Price_Ruble</th>\n",
       "      <th>contract_number</th>\n",
       "      <th>Scopus</th>\n",
       "      <th>Web_of_science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1&amp;m=1</td>\n",
       "      <td>1</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>no suspected english title</td>\n",
       "      <td>57400</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1&amp;m=2</td>\n",
       "      <td>1</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>no suspected english title</td>\n",
       "      <td>53300</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1&amp;m=3</td>\n",
       "      <td>1</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>no suspected english title</td>\n",
       "      <td>49200</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1&amp;m=4</td>\n",
       "      <td>1</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>no suspected english title</td>\n",
       "      <td>45100</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1&amp;m=5</td>\n",
       "      <td>1</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>Прогноз дорожной ситуации на базе статистическ...</td>\n",
       "      <td>no suspected english title</td>\n",
       "      <td>41000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1998&amp;m=3</td>\n",
       "      <td>1998</td>\n",
       "      <td>Влияние возобновляемых источников энергии и эк...</td>\n",
       "      <td>Влияние возобновляемых источников энергии и эк...</td>\n",
       "      <td>Impact of renewables and economic complexity o...</td>\n",
       "      <td>95940</td>\n",
       "      <td>1998.3</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7218</th>\n",
       "      <td>7218</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1998&amp;m=4</td>\n",
       "      <td>1998</td>\n",
       "      <td>Влияние возобновляемых источников энергии и эк...</td>\n",
       "      <td>Влияние возобновляемых источников энергии и эк...</td>\n",
       "      <td>Impact of renewables and economic complexity o...</td>\n",
       "      <td>82000</td>\n",
       "      <td>1998.4</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7219</th>\n",
       "      <td>7219</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1999&amp;m=1</td>\n",
       "      <td>1999</td>\n",
       "      <td>Влияние “3-E” детерминант (экономических, энер...</td>\n",
       "      <td>Влияние “3-E” детерминант (экономических, энер...</td>\n",
       "      <td>Influence of 3-E determinants (economic, energ...</td>\n",
       "      <td>229600</td>\n",
       "      <td>1999.1</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7220</th>\n",
       "      <td>7220</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1999&amp;m=2</td>\n",
       "      <td>1999</td>\n",
       "      <td>Влияние “3-E” детерминант (экономических, энер...</td>\n",
       "      <td>Влияние “3-E” детерминант (экономических, энер...</td>\n",
       "      <td>Influence of 3-E determinants (economic, energ...</td>\n",
       "      <td>123000</td>\n",
       "      <td>1999.2</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221</th>\n",
       "      <td>7221</td>\n",
       "      <td>http://123mi.ru/1/contract.php?r=1&amp;n=1999&amp;m=3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Влияние “3-E” детерминант (экономических, энер...</td>\n",
       "      <td>Влияние “3-E” детерминант (экономических, энер...</td>\n",
       "      <td>Influence of 3-E determinants (economic, energ...</td>\n",
       "      <td>98400</td>\n",
       "      <td>1999.3</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7222 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                  contract_link  \\\n",
       "0              0     http://123mi.ru/1/contract.php?r=1&n=1&m=1   \n",
       "1              1     http://123mi.ru/1/contract.php?r=1&n=1&m=2   \n",
       "2              2     http://123mi.ru/1/contract.php?r=1&n=1&m=3   \n",
       "3              3     http://123mi.ru/1/contract.php?r=1&n=1&m=4   \n",
       "4              4     http://123mi.ru/1/contract.php?r=1&n=1&m=5   \n",
       "...          ...                                            ...   \n",
       "7217        7217  http://123mi.ru/1/contract.php?r=1&n=1998&m=3   \n",
       "7218        7218  http://123mi.ru/1/contract.php?r=1&n=1998&m=4   \n",
       "7219        7219  http://123mi.ru/1/contract.php?r=1&n=1999&m=1   \n",
       "7220        7220  http://123mi.ru/1/contract.php?r=1&n=1999&m=2   \n",
       "7221        7221  http://123mi.ru/1/contract.php?r=1&n=1999&m=3   \n",
       "\n",
       "      Contract_number                                         Base_title  \\\n",
       "0                   1  Прогноз дорожной ситуации на базе статистическ...   \n",
       "1                   1  Прогноз дорожной ситуации на базе статистическ...   \n",
       "2                   1  Прогноз дорожной ситуации на базе статистическ...   \n",
       "3                   1  Прогноз дорожной ситуации на базе статистическ...   \n",
       "4                   1  Прогноз дорожной ситуации на базе статистическ...   \n",
       "...               ...                                                ...   \n",
       "7217             1998  Влияние возобновляемых источников энергии и эк...   \n",
       "7218             1998  Влияние возобновляемых источников энергии и эк...   \n",
       "7219             1999  Влияние “3-E” детерминант (экономических, энер...   \n",
       "7220             1999  Влияние “3-E” детерминант (экономических, энер...   \n",
       "7221             1999  Влияние “3-E” детерминант (экономических, энер...   \n",
       "\n",
       "                                            First_title  \\\n",
       "0     Прогноз дорожной ситуации на базе статистическ...   \n",
       "1     Прогноз дорожной ситуации на базе статистическ...   \n",
       "2     Прогноз дорожной ситуации на базе статистическ...   \n",
       "3     Прогноз дорожной ситуации на базе статистическ...   \n",
       "4     Прогноз дорожной ситуации на базе статистическ...   \n",
       "...                                                 ...   \n",
       "7217  Влияние возобновляемых источников энергии и эк...   \n",
       "7218  Влияние возобновляемых источников энергии и эк...   \n",
       "7219  Влияние “3-E” детерминант (экономических, энер...   \n",
       "7220  Влияние “3-E” детерминант (экономических, энер...   \n",
       "7221  Влияние “3-E” детерминант (экономических, энер...   \n",
       "\n",
       "                                 Suspected_second_title  Price_Ruble  \\\n",
       "0                            no suspected english title        57400   \n",
       "1                            no suspected english title        53300   \n",
       "2                            no suspected english title        49200   \n",
       "3                            no suspected english title        45100   \n",
       "4                            no suspected english title        41000   \n",
       "...                                                 ...          ...   \n",
       "7217  Impact of renewables and economic complexity o...        95940   \n",
       "7218  Impact of renewables and economic complexity o...        82000   \n",
       "7219  Influence of 3-E determinants (economic, energ...       229600   \n",
       "7220  Influence of 3-E determinants (economic, energ...       123000   \n",
       "7221  Influence of 3-E determinants (economic, energ...        98400   \n",
       "\n",
       "      contract_number Scopus  Web_of_science  \n",
       "0                 1.1     Q2           False  \n",
       "1                 1.2     Q2           False  \n",
       "2                 1.3     Q2           False  \n",
       "3                 1.4     Q2           False  \n",
       "4                 1.5     Q2           False  \n",
       "...               ...    ...             ...  \n",
       "7217           1998.3     Q2           False  \n",
       "7218           1998.4     Q2           False  \n",
       "7219           1999.1     Q2           False  \n",
       "7220           1999.2     Q2           False  \n",
       "7221           1999.3     Q2           False  \n",
       "\n",
       "[7222 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://123mi.ru/1/?id=2'\n",
    "html = urllib.request.urlopen(URL)\n",
    "        \n",
    "# Parse the html file\n",
    "htmlParse = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = pd.DataFrame()\n",
    "lis = list()\n",
    "for par in htmlParse.find_all(\"strong\"):\n",
    "    #print(par.get_text())\n",
    "    if '#' in par.get_text():\n",
    "        num = par.get_text()\n",
    "        num = num.strip('#')\n",
    "        data = {'Contract_number':[num]}\n",
    "        #print(data)\n",
    "        df = pd.DataFrame(data) \n",
    "        df_id = df_id.append(df)\n",
    "        lis.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = pd.DataFrame(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = df_cont.rename(columns = {0 : \"Contract_number\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont['In_Website'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont['Contract_number']=df_cont['Contract_number'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paper = df_paper.merge(df_cont, on='Contract_number', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbb7xEDsr6H4",
    "tags": []
   },
   "source": [
    "# Backup of contracts\n",
    "\n",
    "This creates a backup of all the contracts and adds them to the file it was ran in, I would recommend making a file for them to be stored because it is about 10,000 files. This is also not nessary and can be removed if nessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for contract_number in range(1, 2300):\n",
    "    print(contract_number)\n",
    "    # Set the number of author positions to pull\n",
    "    for author_position in range(1,6):\n",
    "        # The starting link is http://123mi.ru/1/contract.php?r=1&n=1&m=1\n",
    "        # A backup for the web archive is: https://web.archive.org/web/20201031161224/http://123mi.ru/1/contract.php?r=1&n=1000&m=1\n",
    "        # This changes the link every loop to ajust to what we need. \n",
    "        url = 'http://123mi.ru/1/contract.php?r=1&n={}&m={}'.format(contract_number, author_position)\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        name = 'Contract{}Position{}.html'.format(contract_number, author_position)\n",
    "        open(name, 'wb').write(r.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cztdlmRoIWpg"
   ],
   "name": "Research_Fraud.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
